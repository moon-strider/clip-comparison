{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import clip\n",
    "from torch.utils.data import Dataset, DataLoader, BatchSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from matplotlib.pyplot import imshow\n",
    "import torchtext\n",
    "import nltk, re, string, collections\n",
    "from nltk.util import ngrams\n",
    "import collections\n",
    "%matplotlib inline\n",
    "BATCH_SIZE = 128\n",
    "EPOCH = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_per_process_memory_fraction(0.5, 0)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: ./input/meme-project-raw/absent-minded-looch.jpg\n",
      "501: ./input/meme-project-raw/oblivious-activist-goat.jpg\n",
      "1001: ./input/meme-project-raw/typical-boy.jpg\n",
      "1501: ./input/meme-project-raw/guess-who-you.jpg\n",
      "2001: ./input/meme-project-raw/dr-steve-brule.jpg\n",
      "2501: ./input/meme-project-raw/vip2-gayle.jpg\n",
      "1: ./input/meme-raw-seg/absent-minded-looch.jpg\n",
      "501: ./input/meme-raw-seg/neymarin.jpg\n",
      "1001: ./input/meme-raw-seg/zayn-malik1.jpg\n",
      "1501: ./input/meme-raw-seg/kd-you-the-real-mvp-f.jpg\n",
      "2001: ./input/meme-raw-seg/svobodus-vulgaris.jpg\n",
      "2501: ./input/meme-raw-seg/this-is-stas.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3000, 2535)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG_ROOT = \"./input/meme-project-raw\"\n",
    "IMG_SEG_ROOT = \"./input/meme-raw-seg\"\n",
    "JSON_ROOT = \"./input/meme-project-clean-json\"\n",
    "img_paths = glob.glob(os.path.join(IMG_ROOT, \"*.jpg\"))\n",
    "img_seg_paths = glob.glob(os.path.join(IMG_SEG_ROOT, \"*.jpg\"))\n",
    "\n",
    "d = {}\n",
    "d_seg = {}\n",
    "\n",
    "for img_path in img_paths:\n",
    "    name = img_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    if not os.path.exists(os.path.join(IMG_ROOT, name + \".json\")):\n",
    "        continue\n",
    "    with open(os.path.join(JSON_ROOT, name+\".json\"), \"r\") as f:\n",
    "        if len(d) / 500 in range((len(d) // 500) + 1):\n",
    "            print(f\"{len(d) + 1}: {os.path.join(IMG_ROOT, name + '.jpg')}\")\n",
    "        captions = json.load(f)\n",
    "        temp = []\n",
    "        for cap in captions:\n",
    "            if \"http\" not in (cap[0] + ' ' + cap[1]) and len(cap[0] + ' ' + cap[1]) >= 8 and len(cap[0] + ' ' + cap[1]) <= 72:\n",
    "                temp.append(cap[0] + ' ' + cap[1])\n",
    "        d[img_path] = temp\n",
    "        \n",
    "i = 0\n",
    "for img_path in img_seg_paths:\n",
    "    i += 1\n",
    "    name = img_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    if not os.path.exists(os.path.join(JSON_ROOT, name + \".json\")):\n",
    "        print(os.path.join(IMG_SEG_ROOT, name + \".json not found, skipping\"))\n",
    "        img_seg_paths.remove(os.path.join(IMG_SEG_ROOT, name + \".jpg\"))\n",
    "        continue\n",
    "    with open(os.path.join(JSON_ROOT, name + \".json\"), \"r\") as f:\n",
    "        if len(d_seg) / 500 in range((len(d_seg) // 500) + 1):\n",
    "            print(f\"{len(d_seg) + 1}: {os.path.join(IMG_SEG_ROOT, name + '.jpg')}\")\n",
    "        captions = json.load(f)\n",
    "        temp = []\n",
    "        for cap in captions:\n",
    "            if \"http\" not in (cap[0]+ ' '+cap[1]) and len(cap[0]+ ' '+cap[1]) >= 8 and len(cap[0]+ ' '+cap[1]) <= 72:\n",
    "                temp.append(cap[0]+ ' '+cap[1])\n",
    "        d_seg[img_path] = temp\n",
    "    if len(d_seg.keys()) != i:\n",
    "        print(f\"{img_path} \\t{name} failed to load json\")\n",
    "        \n",
    "len(d), len(d_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting 20% for Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base split: 2400, 600\n",
      "custom split: 2028, 507\n"
     ]
    }
   ],
   "source": [
    "train_img_paths, test_img_paths = train_test_split(img_paths, test_size=0.2, random_state=42)\n",
    "train_img_seg_paths, test_img_seg_paths = train_test_split(img_seg_paths, test_size=0.2, random_state=42)\n",
    "\n",
    "d_train = {k: d[k] for k in train_img_paths}\n",
    "d_test = {k: d[k] for k in test_img_paths}\n",
    "\n",
    "d_seg_train = {k: d_seg[k] for k in train_img_seg_paths}\n",
    "d_seg_test = {k: d_seg[k] for k in test_img_seg_paths}\n",
    "\n",
    "print(f\"base split: {len(d_train)}, {len(d_test)}\")\n",
    "print(f\"custom split: {len(d_seg_train)}, {len(d_seg_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pre-trained CLIP Model and Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 5.81 GiB total capacity; 4.27 GiB already allocated; 21.06 MiB free; 2.90 GiB allowed; 4.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ilia/Documents/Projects/CLIP/main.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ilia/Documents/Projects/CLIP/main.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ilia/Documents/Projects/CLIP/main.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model, preprocess \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mViT-B/32\u001b[39;49m\u001b[39m\"\u001b[39;49m, device\u001b[39m=\u001b[39;49mdevice, jit\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ilia/Documents/Projects/CLIP/main.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model_1e, preprocess_1e \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mViT-B/32\u001b[39m\u001b[39m\"\u001b[39m, device\u001b[39m=\u001b[39mdevice, jit\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ilia/Documents/Projects/CLIP/main.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model_cs, preprocess_cs \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mViT-B/32\u001b[39m\u001b[39m\"\u001b[39m, device\u001b[39m=\u001b[39mdevice, jit\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/clip/clip.py:139\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, device, jit, download_root)\u001b[0m\n\u001b[1;32m    136\u001b[0m         state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(opened_file, map_location\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m jit:\n\u001b[0;32m--> 139\u001b[0m     model \u001b[39m=\u001b[39m build_model(state_dict \u001b[39mor\u001b[39;49;00m model\u001b[39m.\u001b[39;49mstate_dict())\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    140\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(device) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    141\u001b[0m         model\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 579 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 5.81 GiB total capacity; 4.27 GiB already allocated; 21.06 MiB free; 2.90 GiB allowed; 4.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "model_1e, preprocess_1e = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "model_cs, preprocess_cs = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "\n",
    "image = preprocess(Image.open(\"./input/meme-project-raw/-okay-.jpg\")).unsqueeze(0).to(device)\n",
    "image_1e = preprocess_1e(Image.open(\"./input/meme-project-raw/-okay-.jpg\")).unsqueeze(0).to(device)\n",
    "image_cs = preprocess_cs(Image.open(\"./input/meme-project-raw/-okay-.jpg\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n",
    "print(f\"{image_1e.shape}, {image_cs.shape}, {text.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MemeDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238418, 60319, (tensor([[[ 0.1493, -0.4346, -1.2813,  ...,  1.9011,  1.2150,  0.6165],\n",
      "         [ 0.9376,  0.3099, -0.6390,  ...,  1.4632,  0.5289, -0.1280],\n",
      "         [ 1.8281,  1.1274,  0.2077,  ...,  0.5289, -0.3470, -1.1499],\n",
      "         ...,\n",
      "         [-1.3689, -0.8142,  0.0325,  ...,  0.3683,  1.3026,  1.8573],\n",
      "         [-0.5514,  0.2223,  0.9814,  ..., -0.5514,  0.3391,  1.0252],\n",
      "         [-0.0550,  0.7917,  1.7114,  ..., -1.2083, -0.2156,  0.5143]],\n",
      "\n",
      "        [[ 0.6341,  1.6397,  2.0449,  ..., -1.2568, -0.8666,  0.0038],\n",
      "         [-0.5815,  0.6041,  1.6997,  ..., -0.9867,  0.0789,  1.2945],\n",
      "         [-1.2268, -0.7616,  0.4991,  ..., -0.0262,  1.3845,  1.9848],\n",
      "         ...,\n",
      "         [ 2.0599,  1.8348,  0.7992,  ...,  0.2439, -0.8967, -1.2568],\n",
      "         [ 1.6697,  0.7842, -0.5965,  ...,  1.5946,  0.3040, -0.7016],\n",
      "         [ 0.8593, -0.3864, -1.1818,  ...,  2.0299,  1.3995,  0.2139]],\n",
      "\n",
      "        [[-1.2669, -1.1105, -1.3380,  ..., -1.1247, -1.3522, -1.2527],\n",
      "         [-1.3522, -1.1532, -1.1532,  ..., -1.2243, -1.2669, -1.0678],\n",
      "         [-1.1532, -1.2954, -1.2243,  ..., -1.2811, -1.1247, -1.2954],\n",
      "         ...,\n",
      "         [-1.3949, -1.1958, -1.1958,  ..., -1.2669, -1.3096, -1.1532],\n",
      "         [-1.1389, -1.0963, -1.3096,  ..., -1.1816, -1.2811, -1.3665],\n",
      "         [-1.2385, -1.3238, -1.1958,  ..., -1.3238, -1.0821, -1.2100]]]), \"I'M A TEACHER BUT I HAVE NO PUPILS\", 0)\n",
      "195683, 49376, (tensor([[[ 0.0909,  0.0909,  0.0909,  ..., -1.7777, -1.7777, -1.7777],\n",
      "         [ 0.0909,  0.0909,  0.0909,  ..., -1.7777, -1.7777, -1.7777],\n",
      "         [ 0.0617,  0.0617,  0.0617,  ..., -1.7923, -1.7923, -1.7923],\n",
      "         ...,\n",
      "         [-1.7923, -1.7923, -1.7923,  ...,  0.0617,  0.0617,  0.0617],\n",
      "         [-1.7777, -1.7777, -1.7777,  ...,  0.0617,  0.0617,  0.0617],\n",
      "         [-1.7923, -1.7923, -1.7923,  ...,  0.0471,  0.0617,  0.0763]],\n",
      "\n",
      "        [[ 2.0599,  2.0599,  2.0599,  ...,  0.1989,  0.1989,  0.1989],\n",
      "         [ 2.0749,  2.0749,  2.0749,  ...,  0.1989,  0.1989,  0.1989],\n",
      "         [ 2.0749,  2.0749,  2.0749,  ...,  0.1689,  0.1689,  0.1689],\n",
      "         ...,\n",
      "         [ 0.1689,  0.1689,  0.1689,  ...,  2.0749,  2.0749,  2.0749],\n",
      "         [ 0.1689,  0.1839,  0.1689,  ...,  2.0749,  2.0749,  2.0749],\n",
      "         [ 0.1839,  0.1689,  0.1839,  ...,  2.0599,  2.0599,  2.0599]],\n",
      "\n",
      "        [[ 2.1317,  2.1317,  2.1317,  ...,  2.1317,  2.1317,  2.1317],\n",
      "         [ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459],\n",
      "         [ 2.1317,  2.1317,  2.1317,  ...,  2.1459,  2.1459,  2.1459],\n",
      "         ...,\n",
      "         [ 2.1459,  2.1459,  2.1459,  ...,  2.1317,  2.1317,  2.1317],\n",
      "         [ 2.1459,  2.1459,  2.1459,  ...,  2.1317,  2.1317,  2.1317],\n",
      "         [ 2.1317,  2.1459,  2.1317,  ...,  2.1175,  2.1175,  2.1175]]]), 'WHAT DA FUCK IS THIS SHIT? ', 0)\n",
      "195683, 49376, (tensor([[[ 0.0909,  0.0909,  0.0909,  ..., -1.7777, -1.7777, -1.7777],\n",
      "         [ 0.0909,  0.0909,  0.0909,  ..., -1.7777, -1.7777, -1.7777],\n",
      "         [ 0.0617,  0.0617,  0.0617,  ..., -1.7923, -1.7923, -1.7923],\n",
      "         ...,\n",
      "         [-1.7923, -1.7923, -1.7923,  ...,  0.0617,  0.0617,  0.0617],\n",
      "         [-1.7777, -1.7777, -1.7777,  ...,  0.0617,  0.0617,  0.0617],\n",
      "         [-1.7923, -1.7923, -1.7923,  ...,  0.0471,  0.0617,  0.0763]],\n",
      "\n",
      "        [[ 2.0599,  2.0599,  2.0599,  ...,  0.1989,  0.1989,  0.1989],\n",
      "         [ 2.0749,  2.0749,  2.0749,  ...,  0.1989,  0.1989,  0.1989],\n",
      "         [ 2.0749,  2.0749,  2.0749,  ...,  0.1689,  0.1689,  0.1689],\n",
      "         ...,\n",
      "         [ 0.1689,  0.1689,  0.1689,  ...,  2.0749,  2.0749,  2.0749],\n",
      "         [ 0.1689,  0.1839,  0.1689,  ...,  2.0749,  2.0749,  2.0749],\n",
      "         [ 0.1839,  0.1689,  0.1839,  ...,  2.0599,  2.0599,  2.0599]],\n",
      "\n",
      "        [[ 2.1317,  2.1317,  2.1317,  ...,  2.1317,  2.1317,  2.1317],\n",
      "         [ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459],\n",
      "         [ 2.1317,  2.1317,  2.1317,  ...,  2.1459,  2.1459,  2.1459],\n",
      "         ...,\n",
      "         [ 2.1459,  2.1459,  2.1459,  ...,  2.1317,  2.1317,  2.1317],\n",
      "         [ 2.1459,  2.1459,  2.1459,  ...,  2.1317,  2.1317,  2.1317],\n",
      "         [ 2.1317,  2.1459,  2.1317,  ...,  2.1175,  2.1175,  2.1175]]]), 'WHAT DA FUCK IS THIS SHIT? ', 0)\n"
     ]
    }
   ],
   "source": [
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, data, preprocess):\n",
    "        self.preprocess = preprocess\n",
    "        self.img_paths = []\n",
    "        self.captions = []\n",
    "        for img_path, captions in data.items():\n",
    "            for cap in captions:\n",
    "                self.img_paths.append(img_path)\n",
    "                self.captions.append(cap)\n",
    "        self.processed_cache = {}\n",
    "        for img_path in data:\n",
    "            self.processed_cache[img_path] = self.preprocess(Image.open(img_path))\n",
    "        self.img_paths_set = list(data.keys())\n",
    "        self.path2label = {path: self.img_paths_set.index(path) for path in self.img_paths_set}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = self.processed_cache[img_path]\n",
    "        caption = self.captions[idx]\n",
    "        label = self.path2label[img_path]\n",
    "        return image, caption, label\n",
    "\n",
    "train_dataset = MemeDataset(d_train, preprocess)\n",
    "test_dataset = MemeDataset(d_test, preprocess)\n",
    "\n",
    "train_seg_dataset = MemeDataset(d_seg_train, preprocess_cs)\n",
    "test_seg_dataset = MemeDataset(d_seg_test, preprocess_cs)\n",
    "\n",
    "train_1e_dataset = MemeDataset(d_seg_train, preprocess_1e)  # Лишнее?\n",
    "test_1e_dataset = MemeDataset(d_seg_test, preprocess_1e)\n",
    "\n",
    "print(f\"{len(train_dataset)}, {len(test_dataset)}, {train_dataset[0]}\")\n",
    "print(f\"{len(train_seg_dataset)}, {len(test_seg_dataset)}, {train_seg_dataset[0]}\")\n",
    "print(f\"{len(train_1e_dataset)}, {len(test_1e_dataset)}, {train_1e_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) ./input/meme-raw-seg/super-ssau.jpg, 0\n",
      "2) ./input/meme-raw-seg/female-internet-troll.jpg, 1\n",
      "3) ./input/meme-raw-seg/y-u-no-bren-iwsnt.jpg, 2\n",
      "4) ./input/meme-raw-seg/how-tough-are-you.jpg, 3\n",
      "5) ./input/meme-raw-seg/dinosaur-director.jpg, 4\n",
      "6) ./input/meme-raw-seg/kanyetothe.jpg, 5\n",
      "7) ./input/meme-raw-seg/arnaldo-tirone.jpg, 6\n",
      "8) ./input/meme-raw-seg/creepy-ash.jpg, 7\n",
      "9) ./input/meme-raw-seg/reborn-logic.jpg, 8\n",
      "10) ./input/meme-raw-seg/hoodie-faggot.jpg, 9\n",
      "1) ./input/meme-raw-seg/super-ssau.jpg, 0\n",
      "2) ./input/meme-raw-seg/female-internet-troll.jpg, 1\n",
      "3) ./input/meme-raw-seg/y-u-no-bren-iwsnt.jpg, 2\n",
      "4) ./input/meme-raw-seg/how-tough-are-you.jpg, 3\n",
      "5) ./input/meme-raw-seg/dinosaur-director.jpg, 4\n",
      "6) ./input/meme-raw-seg/kanyetothe.jpg, 5\n",
      "7) ./input/meme-raw-seg/arnaldo-tirone.jpg, 6\n",
      "8) ./input/meme-raw-seg/creepy-ash.jpg, 7\n",
      "9) ./input/meme-raw-seg/reborn-logic.jpg, 8\n",
      "10) ./input/meme-raw-seg/hoodie-faggot.jpg, 9\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for k,v in train_seg_dataset.path2label.items():\n",
    "    i+=1\n",
    "    print(f\"{i}) {k}, {v}\")\n",
    "    if i == 10:\n",
    "        break\n",
    "        \n",
    "i = 0\n",
    "for k,v in train_1e_dataset.path2label.items():\n",
    "    i+=1\n",
    "    print(f\"{i}) {k}, {v}\")\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BalancedBatchSampler (ensures no same class per batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedBatchSampler(BatchSampler):\n",
    "    \"\"\"\n",
    "    BatchSampler - from a MNIST-like dataset, samples n_classes and within these classes samples n_samples.\n",
    "    Returns batches of size n_classes * n_samples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, n_classes, n_samples):\n",
    "        self.labels = labels\n",
    "        self.labels_set = list(set(self.labels.numpy()))\n",
    "        self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n",
    "                                 for label in self.labels_set}\n",
    "        for l in self.labels_set:\n",
    "            np.random.shuffle(self.label_to_indices[l])\n",
    "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
    "        self.count = 0\n",
    "        self.n_classes = n_classes\n",
    "        self.n_samples = n_samples\n",
    "        self.n_dataset = len(self.labels)\n",
    "        self.batch_size = self.n_samples * self.n_classes\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        while self.count + self.batch_size < self.n_dataset:\n",
    "            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
    "            indices = []\n",
    "            for class_ in classes:\n",
    "                indices.extend(self.label_to_indices[class_][\n",
    "                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
    "                                                                         class_] + self.n_samples])\n",
    "                self.used_label_indices_count[class_] += self.n_samples\n",
    "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
    "                    np.random.shuffle(self.label_to_indices[class_])\n",
    "                    self.used_label_indices_count[class_] = 0\n",
    "            yield indices\n",
    "            self.count += self.n_classes * self.n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_dataset // self.batch_size\n",
    "    \n",
    "train_labels = torch.tensor([item[2] for item in train_dataset])\n",
    "train_sampler = BalancedBatchSampler(train_labels, BATCH_SIZE, 1)\n",
    "train_dataloader = DataLoader(train_dataset, batch_sampler=train_sampler)\n",
    "\n",
    "test_labels = torch.tensor([item[2] for item in test_dataset])\n",
    "test_sampler = BalancedBatchSampler(test_labels, BATCH_SIZE, 1)\n",
    "test_dataloader = DataLoader(test_dataset, batch_sampler=test_sampler)\n",
    "\n",
    "train_seg_labels = torch.tensor([item[2] for item in train_seg_dataset])\n",
    "train_seg_sampler = BalancedBatchSampler(train_seg_labels, BATCH_SIZE, 1)\n",
    "train_seg_dataloader = DataLoader(train_seg_dataset, batch_sampler=train_seg_sampler)\n",
    "\n",
    "test_seg_labels = torch.tensor([item[2] for item in test_seg_dataset])\n",
    "test_seg_sampler = BalancedBatchSampler(test_seg_labels, BATCH_SIZE, 1)\n",
    "test_seg_dataloader = DataLoader(test_seg_dataset, batch_sampler=test_seg_sampler)\n",
    "\n",
    "train_1e_labels = torch.tensor([item[2] for item in train_1e_dataset])\n",
    "train_1e_sampler = BalancedBatchSampler(train_1e_labels, BATCH_SIZE, 1)\n",
    "train_1e_dataloader = DataLoader(train_1e_dataset, batch_sampler=train_1e_sampler)\n",
    "\n",
    "test_1e_labels = torch.tensor([item[2] for item in test_1e_dataset])\n",
    "test_1e_sampler = BalancedBatchSampler(test_1e_labels, BATCH_SIZE, 1)\n",
    "test_1e_dataloader = DataLoader(test_1e_dataset, batch_sampler=test_1e_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128, 128\n",
      "128, 128\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(train_seg_sampler):\n",
    "    labels = []\n",
    "    for idx in item:\n",
    "        label = train_seg_dataset[idx][2]\n",
    "        labels.append(label)\n",
    "    break\n",
    "    \n",
    "for i, item in enumerate(train_1e_sampler):\n",
    "    labels1e = []\n",
    "    for idx in item:\n",
    "        label = train_1e_dataset[idx][2]\n",
    "        labels1e.append(label)\n",
    "    break\n",
    "    \n",
    "print(f\"{len(labels)}, {len(set(labels))}\")\n",
    "print(f\"{len(labels1e)}, {len(set(labels1e))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seg:\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "tensor([ 309, 1052,  944,  577, 1449, 1614, 1491, 1279, 1049,  688, 1267,  855,\n",
      "         415,  686, 1686, 1479, 1219,  971, 1566,  397, 1330, 1047, 1395,  586,\n",
      "         177,  843,  930,  274,  326,  511, 1385,  488, 1380,  687, 1378, 1190,\n",
      "        1936,  654,  950, 1748,  368, 2011,  232,  347,  278, 1407, 1698, 1468,\n",
      "        1241, 2015, 1349, 1188,  866,  306,  926,  446,  775, 2023, 1605,  510,\n",
      "         531, 1680, 1299,  393, 1909, 1773, 1597,   92, 1160,  621,   21,   69,\n",
      "         160, 1558,  130, 1193, 1620, 1564, 1362, 1311, 1735, 1266, 1418, 1278,\n",
      "        1465,  118, 1280, 1903, 1017, 1743,  254, 1083,  199, 1302,  951,  429,\n",
      "         186, 1990, 1414, 1634,  502, 1112,  602,  471,  333,  994,  757,  784,\n",
      "        1926, 1596,  884,  509,  165,  715, 1599,  494,  829,  230, 1980,  476,\n",
      "        1423, 1677, 2004,  872, 1216, 1158, 1469, 1272])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "\n",
      "1e:\n",
      "torch.Size([128, 3, 224, 224])\n",
      "128\n",
      "tensor([1203, 1333,  483,  511, 1191,  666, 1628,  669, 1465,  208,  583, 1492,\n",
      "         497,   69,  185, 1010, 1898, 1773,  283,  821, 1858, 1260, 1189,  241,\n",
      "         141, 1579,  370,  275, 1841, 1140, 1928, 1738,   25,  499,   11, 1505,\n",
      "        1393, 1758,  555,  739,  792,  591,  810,  153,  755, 1719,  179,  767,\n",
      "        1034, 1391, 1069, 1741,  772, 1595, 1368, 1640,  938,  675, 1309,    0,\n",
      "        1100,  245, 1471, 1564,  984, 1077,  344, 1830, 1669, 1706, 1590,  551,\n",
      "        1484, 2007,  919, 1099,  918,  728, 2025, 2000, 1740,  837,   94,   10,\n",
      "         123,  715,  626,  925, 1383,  804,  980,  399,   43,  202, 1703,  871,\n",
      "         464, 1346,  877, 1490, 1047, 1403, 1138, 1610, 1143,   98,  568,  481,\n",
      "        1302, 1453,  255, 1621,  263, 1046, 1438, 1090,  525,  452, 1612,  191,\n",
      "         529, 1986,  807,  674, 1681, 1030, 1301, 1771])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(\"seg:\")\n",
    "for batch in train_seg_dataloader:\n",
    "    imgs, txts, labels = batch\n",
    "    print(imgs.shape)\n",
    "    print(len(txts))\n",
    "    print(labels)\n",
    "    print(labels.shape)\n",
    "    print(torch.unique(labels).shape)\n",
    "    break\n",
    "    \n",
    "print(\"\\n1e:\")\n",
    "for batch in train_1e_dataloader:\n",
    "    imgs, txts, labels = batch\n",
    "    print(imgs.shape)\n",
    "    print(len(txts))\n",
    "    print(labels)\n",
    "    print(labels.shape)\n",
    "    print(torch.unique(labels).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "if device == \"cpu\":\n",
    "    model.float()\n",
    "    model_1e.float()\n",
    "    model_cs.float()\n",
    "\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_dataloader)*EPOCH)\n",
    "\n",
    "optimizer_1e = optim.Adam(model_1e.parameters(), lr=1e-5)\n",
    "scheduler_1e = optim.lr_scheduler.CosineAnnealingLR(optimizer_1e, len(train_1e_dataloader)*1)\n",
    "\n",
    "optimizer_cs = optim.Adam(model_cs.parameters(), lr=1e-5)\n",
    "scheduler_cs = optim.lr_scheduler.CosineAnnealingLR(optimizer_cs, len(train_seg_dataloader)*EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initTraining(model_cs, \"cs\", 4) => обучение модели model_cs, обученные файлы будут сохраняться с добавлением аппендикса \"cs\",\n",
    "# обучение будет длиться 4 эпохи.\n",
    "def initTraining(model, train_dataloader, test_dataloader, optimizer, scheduler, appendix=\"base\", EPOCH=EPOCH):\n",
    "    best_te_loss = 1e5\n",
    "    best_ep = -1\n",
    "    for epoch in range(EPOCH): # Здесь нужно обучать cs модель, посмотреть циклы ниже\n",
    "        print(f\"running epoch {epoch}, best test loss {best_te_loss} after epoch {best_ep}\")\n",
    "        step = 0\n",
    "        tr_loss = 0\n",
    "        model.train()\n",
    "        pbar = tqdm(train_dataloader, leave=False)\n",
    "        for batch in pbar:\n",
    "            step += 1\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            images, texts, _ = batch\n",
    "            images = images.to(device)\n",
    "            texts = clip.tokenize(texts).to(device)\n",
    "    #       print(images.shape, texts.shape)\n",
    "            logits_per_image, logits_per_text = model(images, texts)\n",
    "            ground_truth = torch.arange(BATCH_SIZE).to(device)\n",
    "\n",
    "            total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "            total_loss.backward()\n",
    "            tr_loss += total_loss.item()\n",
    "            if device == \"cpu\":\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "            else:\n",
    "                convert_models_to_fp32(model)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                clip.model.convert_weights(model)\n",
    "            pbar.set_description(f\"train batchCE: {total_loss.item()}\", refresh=True)\n",
    "        tr_loss /= step\n",
    "\n",
    "        step = 0\n",
    "        te_loss = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            test_pbar = tqdm(test_dataloader, leave=False)\n",
    "            for batch in test_pbar:\n",
    "                step += 1\n",
    "                images, texts, _ = batch\n",
    "                images = images.to(device)\n",
    "                texts = clip.tokenize(texts).to(device)\n",
    "                logits_per_image, logits_per_text = model(images, texts)\n",
    "                ground_truth = torch.arange(BATCH_SIZE).to(device)\n",
    "\n",
    "                total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "                te_loss += total_loss.item()\n",
    "                test_pbar.set_description(f\"test batchCE: {total_loss.item()}\", refresh=True)\n",
    "            te_loss /= step\n",
    "\n",
    "        if te_loss < best_te_loss:\n",
    "            best_te_loss = te_loss\n",
    "            best_ep = epoch\n",
    "            torch.save(model.state_dict(), f\"best_{appendix}_model.pt\")\n",
    "        print(f\"epoch {epoch}, tr_loss {tr_loss}, te_loss {te_loss}\")\n",
    "    torch.save(model.state_dict(), f\"last_{appendix}_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initTraining(model_cs, train_seg_dataloader, test_seg_dataloader, optimizer_cs, scheduler_cs, \"cs\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running epoch 0, best test loss 100000.0 after epoch -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                        \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 5.81 GiB total capacity; 4.17 GiB already allocated; 21.06 MiB free; 2.90 GiB allowed; 4.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ilia/Documents/Projects/CLIP/main.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ilia/Documents/Projects/CLIP/main.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m initTraining(model_1e, train_1e_dataloader, test_1e_dataloader, optimizer_1e, scheduler_1e, \u001b[39m\"\u001b[39;49m\u001b[39m1e\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/home/ilia/Documents/Projects/CLIP/main.ipynb Cell 22\u001b[0m in \u001b[0;36minitTraining\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, scheduler, appendix, EPOCH)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ilia/Documents/Projects/CLIP/main.ipynb#X26sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         texts \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39mtokenize(texts)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ilia/Documents/Projects/CLIP/main.ipynb#X26sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#       print(images.shape, texts.shape)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ilia/Documents/Projects/CLIP/main.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         logits_per_image, logits_per_text \u001b[39m=\u001b[39m model(images, texts)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ilia/Documents/Projects/CLIP/main.ipynb#X26sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         ground_truth \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(BATCH_SIZE)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ilia/Documents/Projects/CLIP/main.ipynb#X26sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         total_loss \u001b[39m=\u001b[39m (loss_img(logits_per_image,ground_truth) \u001b[39m+\u001b[39m loss_txt(logits_per_text,ground_truth))\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/clip/model.py:360\u001b[0m, in \u001b[0;36mCLIP.forward\u001b[0;34m(self, image, text)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, image, text):\n\u001b[1;32m    359\u001b[0m     image_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_image(image)\n\u001b[0;32m--> 360\u001b[0m     text_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_text(text)\n\u001b[1;32m    362\u001b[0m     \u001b[39m# normalized features\u001b[39;00m\n\u001b[1;32m    363\u001b[0m     image_features \u001b[39m=\u001b[39m image_features \u001b[39m/\u001b[39m image_features\u001b[39m.\u001b[39mnorm(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/clip/model.py:348\u001b[0m, in \u001b[0;36mCLIP.encode_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    346\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_embedding\u001b[39m.\u001b[39mtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    347\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# NLD -> LND\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x)\n\u001b[1;32m    349\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# LND -> NLD\u001b[39;00m\n\u001b[1;32m    350\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_final(x)\u001b[39m.\u001b[39mtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/clip/model.py:203\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresblocks(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/clip/model.py:190\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 190\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_1(x))\n\u001b[1;32m    191\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(x))\n\u001b[1;32m    192\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/ds/lib/python3.9/site-packages/clip/model.py:162\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    161\u001b[0m     orig_type \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mdtype\n\u001b[0;32m--> 162\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mforward(x\u001b[39m.\u001b[39;49mtype(torch\u001b[39m.\u001b[39;49mfloat32))\n\u001b[1;32m    163\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39mtype(orig_type)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 5.81 GiB total capacity; 4.17 GiB already allocated; 21.06 MiB free; 2.90 GiB allowed; 4.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "initTraining(model_1e, train_1e_dataloader, test_1e_dataloader, optimizer_1e, scheduler_1e, \"1e\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Precision on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./input/tuned-clips/best_model.pt\"))\n",
    "model_cs.load_state_dict(torch.load(\"./input/tuned-clips/best_cs_model.pt\"))\n",
    "model_1e.load_state_dict(torch.load(\"./input/tuned-clips/best_1e_model.pt\"))\n",
    "NUM_NEG = 127\n",
    "NUM_TEST = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatePrecision(model, preprocess, d_test, appendix=\"base\"):\n",
    "    n_correct = 0\n",
    "    for i in tqdm(range(NUM_TEST)):\n",
    "        empty = True\n",
    "\n",
    "        while empty:\n",
    "            img_path = random.choice(list(d_test.keys()))\n",
    "            image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "            name = img_path.split('/')[-1].split('.')[0]\n",
    "            caps = d_test[img_path]\n",
    "            if len(caps) > 0:\n",
    "                pos_txt = random.choice(caps)\n",
    "                empty = False\n",
    "\n",
    "        neg_i = 0\n",
    "        neg_txts = []\n",
    "        while neg_i < NUM_NEG:\n",
    "            img_path = random.choice(list(d_test.keys()))\n",
    "            neg_name = img_path.split('/')[-1].split('.')[0]\n",
    "            if neg_name == name:\n",
    "                continue\n",
    "            caps = d_test[img_path]\n",
    "            if len(caps) == 0:\n",
    "                continue\n",
    "            neg_txt = random.choice(caps)\n",
    "            if neg_txt in neg_txts:\n",
    "                continue\n",
    "            neg_txts.append(neg_txt)\n",
    "            neg_i += 1\n",
    "\n",
    "        text = clip.tokenize([pos_txt]+neg_txts).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image)\n",
    "            text_features = model.encode_text(text)\n",
    "            logits_per_image, logits_per_text = model(image, text)\n",
    "            probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "        if np.argmax(probs) == 0:\n",
    "            n_correct +=1\n",
    "    print(f\"Test precision on model_{appendix} {n_correct / NUM_TEST}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:57<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test precision on model_base 0.546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:59<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test precision on model_1e 0.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:00<00:00,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test precision on model_cs 0.559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluatePrecision(model, preprocess, d_test, appendix=\"base\")\n",
    "evaluatePrecision(model_1e, preprocess_1e, d_seg_test, appendix=\"1e\")\n",
    "evaluatePrecision(model_cs, preprocess_cs, d_seg_test, appendix=\"cs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating BLEU and Word Diversity using Naive Sampling\n",
    "\n",
    "## Sampling Captions for Validation Images According to CLIP Text-Image Proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample1Caption(img_path, corpus, preprocess, model, num_cand):\n",
    "    image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
    "    i = 0\n",
    "    txts = []\n",
    "    while i < num_cand:\n",
    "        txt = random.choice(corpus)\n",
    "        if txt in txts:\n",
    "            continue\n",
    "        if len(txt.split())<5 or len(txt)>72:\n",
    "            continue\n",
    "        txts.append(txt)\n",
    "        i += 1\n",
    "\n",
    "    text = clip.tokenize(txts).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_per_image, logits_per_text = model(image, text)\n",
    "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "    return txts[np.argmax(probs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238418 I'M A TEACHER BUT I HAVE NO PUPILS\n",
      "195683 WHAT DA FUCK IS THIS SHIT? \n",
      "195683 WHAT DA FUCK IS THIS SHIT? \n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "corpus_cs = []\n",
    "corpus_1e = []\n",
    "for txtlist in d_train.values():\n",
    "    corpus += txtlist\n",
    "for txtlist in d_seg_train.values():\n",
    "    corpus_cs += txtlist\n",
    "    corpus_1e += txtlist\n",
    "print(len(corpus), corpus[0])\n",
    "print(len(corpus_cs), corpus_cs[0])\n",
    "print(len(corpus_1e), corpus_1e[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling for base model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [07:14<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling for cs model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 507/507 [06:13<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling for 1e model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 507/507 [06:09<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"sampling for base model\")\n",
    "captions_base = {}\n",
    "for img_path in tqdm(d_test.keys()):\n",
    "    caption = sample1Caption(img_path, corpus, preprocess, model, 1000)\n",
    "    captions_base[img_path] = caption\n",
    "    \n",
    "print(\"sampling for cs model\")\n",
    "captions_cs = {}\n",
    "for img_path in tqdm(d_seg_test.keys()):\n",
    "    caption = sample1Caption(img_path, corpus_cs, preprocess_cs, model_cs, 1000)\n",
    "    captions_cs[img_path] = caption\n",
    "    \n",
    "print(\"sampling for 1e model\")\n",
    "captions_1e = {}\n",
    "for img_path in tqdm(d_seg_test.keys()):\n",
    "    caption = sample1Caption(img_path, corpus_1e, preprocess_1e, model_1e, 1000)\n",
    "    captions_1e[img_path] = caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateBleuScore(captionst, d_t, appendix=\"base\"):\n",
    "    broken = 0\n",
    "    for get_bleu in range(1,4):\n",
    "        bleu_x_lst = []\n",
    "        bleu_y_lst = []\n",
    "        for p, caps in d_t.items():\n",
    "            if not caps:\n",
    "                continue\n",
    "            if p not in captionst.keys():\n",
    "                broken += 1\n",
    "                continue\n",
    "            bleu_x_lst.append(captionst[p].split())\n",
    "            splittedcaps = [x.split() for x in caps]\n",
    "            bleu_y_lst.append(splittedcaps)\n",
    "        BLEU = torchtext.data.metrics.bleu_score(bleu_x_lst, bleu_y_lst, max_n=get_bleu, weights=[1/get_bleu]*get_bleu)\n",
    "        print(f\"{get_bleu}-gram BLEU score ({appendix}): {BLEU}, broken files: {broken}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600, 507, 507\n",
      "600, 507\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(captions_base)}, {len(captions_cs)}, {len(captions_1e)}\")\n",
    "print(f\"{len(d_test.keys())}, {len(d_seg_test.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram BLEU score (base): 0.4022921621799469, broken files: 0\n",
      "2-gram BLEU score (base): 0.17008958756923676, broken files: 0\n",
      "3-gram BLEU score (base): 0.09917695075273514, broken files: 0\n",
      "1-gram BLEU score (cs): 0.3872121572494507, broken files: 0\n",
      "2-gram BLEU score (cs): 0.16243566572666168, broken files: 0\n",
      "3-gram BLEU score (cs): 0.08899804949760437, broken files: 0\n",
      "1-gram BLEU score (1e): 0.38601332902908325, broken files: 0\n",
      "2-gram BLEU score (1e): 0.15987643599510193, broken files: 0\n",
      "3-gram BLEU score (1e): 0.08371224254369736, broken files: 0\n"
     ]
    }
   ],
   "source": [
    "calculateBleuScore(captions_base, d_test, \"base\")\n",
    "calculateBleuScore(captions_cs, d_seg_test, \"cs\")\n",
    "calculateBleuScore(captions_1e, d_seg_test, \"1e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e9a78d78e4970bb4088d64ea99b4a32762e278566aa5bba4cdf8ff14749da40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
